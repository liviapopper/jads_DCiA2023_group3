{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc72d31",
   "metadata": {},
   "source": [
    "# **BERTopic - Tutorial**\n",
    "We start with installing bertopic from pypi before preparing the data. \n",
    "\n",
    "**NOTE**: Make sure to select a GPU runtime. Otherwise, the model can take quite some time to create the document embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea810445",
   "metadata": {},
   "source": [
    "# **Prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faecde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import string, pprint\n",
    "# gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from bertopic import BERTopic\n",
    "import spacy\n",
    "import nl_core_news_sm\n",
    "import ijson\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from bertopic.backend import languages\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e9f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents into a documents list\n",
    "documents = []\n",
    "\n",
    "with open(\"../Source/sample_100.json\", \"rb\") as j:\n",
    "    for record in ijson.items(j, \"item\"):\n",
    "        documents.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d07e321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_titles = []\n",
    "for doc in documents:\n",
    "    doc_titles.append(doc['document_title'])\n",
    "    \n",
    "\n",
    "doc_content = []\n",
    "for doc in documents:\n",
    "    doc_content.append(doc['content'])\n",
    "    \n",
    "data = {'title': doc_titles, 'content': doc_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82a79d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f511116",
   "metadata": {},
   "source": [
    "# **Splitting up the documents into paragraphs**\n",
    "BERT topic modeling based on paragraphs instead of whole documents has several advantages, including:\n",
    "\n",
    "1. Improved granularity: Topic modeling based on paragraphs allows for a more fine-grained analysis of text data. It allows for a better understanding of the themes and topics within a larger document, which can help with more precise and accurate categorization of text data.\n",
    "\n",
    "2. Better representation of content: Analyzing individual paragraphs rather than whole documents can provide a more accurate representation of the content in a given document. This is particularly important for longer documents where the content can vary significantly across different sections.\n",
    "\n",
    "3. Better results for shorter documents: BERT-based topic modeling can be challenging for short documents, as there may not be enough information to generate meaningful topics. Analyzing individual paragraphs can provide more reliable results for shorter documents.\n",
    "\n",
    "4. Ability to identify multiple topics: BERT topic modeling based on paragraphs can help identify multiple topics within a single document, which can be particularly useful in cases where there are multiple themes or subtopics.\n",
    "\n",
    "Overall, BERT topic modeling based on paragraphs can provide a more detailed and accurate analysis of text data compared to analyzing whole documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5676387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "def split_into_paragraphs(text):\n",
    "    nlp = spacy.load('nl_core_news_lg')\n",
    "    doc = nlp(text)\n",
    "    paragraphs = []\n",
    "    current_paragraph = ''\n",
    "    \n",
    "    for sentence in doc.sents:\n",
    "        if len(current_paragraph) == 0:\n",
    "            current_paragraph = str(sentence)\n",
    "        else:\n",
    "            similarity = sentence.similarity(nlp(current_paragraph))\n",
    "            if similarity < 0.6:  # threshold for new paragraph\n",
    "                paragraphs.append(current_paragraph)\n",
    "                current_paragraph = str(sentence)\n",
    "            else:\n",
    "                current_paragraph += '\\n' + str(sentence)\n",
    "\n",
    "    paragraphs.append(current_paragraph)  # add last paragraph\n",
    "    return paragraphs\n",
    "\n",
    "# create new dataframe for paragraphs\n",
    "paragraphsDF = pd.DataFrame(columns=['title', 'paragraph_num', 'paragraph_text'])\n",
    "\n",
    "# loop over documents in the dataset and split each one into paragraphs\n",
    "for i, row in dataDF.iterrows():\n",
    "    title = row['title']\n",
    "    content = row['content']\n",
    "    paragraphs = split_into_paragraphs(content)\n",
    "    \n",
    "    # add each paragraph to the new dataframe\n",
    "    for j, paragraph in enumerate(paragraphs):\n",
    "        paragraphsDF = paragraphsDF.append({'title': title, 'paragraph_num': j+1, 'paragraph_text': paragraph}, ignore_index=True)\n",
    "\n",
    "# save new dataframe to a csv file\n",
    "paragraphsDF.to_csv('paragraphs_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c73aca8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>paragraph_num</th>\n",
       "      <th>paragraph_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>1</td>\n",
       "      <td>&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Retouradres Postbus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>3</td>\n",
       "      <td>20350 2500 EJ Den Haag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>4</td>\n",
       "      <td>De Voorzitter van de Tweede Kamer der Staten-G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Postbus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22324</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>66</td>\n",
       "      <td>Antwoord 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22325</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>67</td>\n",
       "      <td>Banken, pensioenfondsen en andere financiële i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22326</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>68</td>\n",
       "      <td>Vraag 15 Kunt u deze vragen elk afzonderlijk e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22327</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>69</td>\n",
       "      <td>Antwoord 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22328</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>70</td>\n",
       "      <td>Ja.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22329 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title paragraph_num  \\\n",
       "0      Contaminatie in het vlees van ‘grote grazers’ ...             1   \n",
       "1      Contaminatie in het vlees van ‘grote grazers’ ...             2   \n",
       "2      Contaminatie in het vlees van ‘grote grazers’ ...             3   \n",
       "3      Contaminatie in het vlees van ‘grote grazers’ ...             4   \n",
       "4      Contaminatie in het vlees van ‘grote grazers’ ...             5   \n",
       "...                                                  ...           ...   \n",
       "22324  Antwoord op de vragen van het lid Tjeerd de Gr...            66   \n",
       "22325  Antwoord op de vragen van het lid Tjeerd de Gr...            67   \n",
       "22326  Antwoord op de vragen van het lid Tjeerd de Gr...            68   \n",
       "22327  Antwoord op de vragen van het lid Tjeerd de Gr...            69   \n",
       "22328  Antwoord op de vragen van het lid Tjeerd de Gr...            70   \n",
       "\n",
       "                                          paragraph_text  \n",
       "0                                                      >  \n",
       "1                                    Retouradres Postbus  \n",
       "2                                 20350 2500 EJ Den Haag  \n",
       "3      De Voorzitter van de Tweede Kamer der Staten-G...  \n",
       "4                                                Postbus  \n",
       "...                                                  ...  \n",
       "22324                                        Antwoord 14  \n",
       "22325  Banken, pensioenfondsen en andere financiële i...  \n",
       "22326  Vraag 15 Kunt u deze vragen elk afzonderlijk e...  \n",
       "22327                                        Antwoord 15  \n",
       "22328                                                Ja.  \n",
       "\n",
       "[22329 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189abd69",
   "metadata": {},
   "source": [
    "# **Text preprocessing**\n",
    "The preprocessing pipeline is mentioned below.\n",
    "#### 1. Tokenisation\n",
    "First basic tokenization is implemented, to split the text into \n",
    "tokens as is recommended by Kannan et al. (2014). For this process I used genism’s \n",
    "simple_preprocess, which will convert the text into lowercase & tokens and remove punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbde2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization using gensim\n",
    "def sent_to_words(sentences, deacc=True): # deacc=True removes punctuations\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence)))  \n",
    "        \n",
    "# Convert the data to a list\n",
    "data = paragraphsDF[\"paragraph_text\"].values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6893db",
   "metadata": {},
   "source": [
    "#### 2. Stop word removal \n",
    "Secondly stop words will be removed from the data as well as a list of punctuation characters from \n",
    "the string.punctiation string, which is a pre-initialized string used as a string constant. These are \n",
    "removed because they have little relevance for understanding the content of a text (Kannan et al., \n",
    "2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1422ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of additional stop words\n",
    "# We remove additional common words that o# Create stopword list\n",
    "# string.punctuation refers to a list of punctuations\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('dutch') + list(string.punctuation) #occur in many documents and have no link to a distinct industry.\n",
    "additional_stop_words = ['geer','minister','vraag', 'postbus', 'retouradres', 'antwoord']\n",
    "stop_words = stop_words + additional_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "385fdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stopwords from the data\n",
    "def rem_stopwords (text):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in text]\n",
    "\n",
    "# remove stop words\n",
    "data_words_nostops = rem_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb5c3e",
   "metadata": {},
   "source": [
    "#### 3. Lemmatization\n",
    "Lastly lemmatization has been performed, since its superior to stemming (Khyani et al., 2021), \n",
    "which is a text normalization technique that will switch any word to its lemma. For this process I \n",
    "used to open-source software library called spaCy, but NLTK could also have been used. The spaCy \n",
    "pre-trained model called en_core_web_md, can be thought of as some kind of pipeline. When this \n",
    "model is called upon a text or word, the text will run through the pipeline. If the text isn’t tokenized \n",
    "it will be tokenized after which different components will be activated. The thing that’s most \n",
    "interesting about this pipeline is a tagger which will assign Part-of-Speech tags based on spaCy’s \n",
    "English language model. This is done to gain a variety of annotations. The POS tag refers to a label \n",
    "which will be assigned to every token in the corpus to indicate the type of said token (is it a verb or \n",
    "punctation or adjective) and other grammatical categories. These POS tags can then be used in the \n",
    "preprocess to remove unwanted tags. The only tags that I have allowed in my analysis are Noun, Adj, \n",
    "Verb and Adv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4132480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "# or even higher\n",
    "\n",
    "nlp = spacy.load('nl_core_news_lg', disable=['parser', 'ner'])\n",
    "nlp.max_length = 1322782\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d13273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphsDF['content'] = data_lemmatized\n",
    "newDF = paragraphsDF.drop(paragraphsDF[paragraphsDF['content'].apply(lambda x: len(x)==0)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549253f",
   "metadata": {},
   "source": [
    "# **Creating Corpus & BERTopics**\n",
    "BERTopic is a smart topic modeling algorithm that utilizes BERT (Bidirectional Encoder Representations from Transformers), a state-of-the-art natural language processing model developed by Google, to create meaningful and accurate topics from a given corpus. Here are some reasons why BERTopic is considered smart to use:\n",
    "\n",
    "1. Incorporates contextual understanding: BERT is designed to understand the context of text data, which allows BERTopic to create topics that are based on the full context of the documents. This makes it more accurate and meaningful compared to other topic modeling algorithms.\n",
    "\n",
    "2. Utilizes clustering: BERTopic uses clustering algorithms, such as Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN), to group similar documents together and create coherent topics. This clustering approach helps ensure that the topics are not only meaningful but also distinguishable from one another.\n",
    "\n",
    "3. Customizable: BERTopic is highly customizable and can be tailored to specific needs. For example, users can adjust the number of topics they want to extract, or exclude specific words from the analysis to improve the quality of topics generated.\n",
    "\n",
    "4. Efficient: BERTopic is designed to be computationally efficient and can process large datasets quickly. It also has the ability to update topics as new documents are added to the corpus, making it a scalable solution for topic modeling.\n",
    "\n",
    "5. Easy to use: BERTopic is user-friendly and can be implemented with just a few lines of code. The resulting topics can be visualized using a variety of tools, making it easy to interpret and communicate the findings to others.\n",
    "\n",
    "Overall, BERTopic is a smart choice for topic modeling as it combines the power of BERT with efficient clustering algorithms and customizability to create meaningful and accurate topics from text data.\n",
    "\n",
    "We select the \"dutch\" as the main language for our documents. If you want a multilingual model that supports 50+ languages, please select \"multilingual\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11e2f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF['corp'] = [','.join(map(str, l)) for l in newDF['content']]\n",
    "newDF['corp'] = newDF['corp'].str.replace(',',' ', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8b16034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index of the dataframe\n",
    "newDF = newDF.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56dab573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>paragraph_num</th>\n",
       "      <th>paragraph_text</th>\n",
       "      <th>content</th>\n",
       "      <th>corp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>4</td>\n",
       "      <td>De Voorzitter van de Tweede Kamer der Staten-G...</td>\n",
       "      <td>[voorzitter, generaal]</td>\n",
       "      <td>voorzitter generaal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>7</td>\n",
       "      <td>: Parnassusplein 5</td>\n",
       "      <td>[parnassusplein]</td>\n",
       "      <td>parnassusplein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>9</td>\n",
       "      <td>www.rijksoverheid.nl</td>\n",
       "      <td>[www, rijksoverheid]</td>\n",
       "      <td>www rijksoverheid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>10</td>\n",
       "      <td>Kenmerk 3477122-1040635-VGP</td>\n",
       "      <td>[Kenmerk]</td>\n",
       "      <td>Kenmerk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contaminatie in het vlees van ‘grote grazers’ ...</td>\n",
       "      <td>11</td>\n",
       "      <td>Uw brief</td>\n",
       "      <td>[brief]</td>\n",
       "      <td>brief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15942</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>61</td>\n",
       "      <td>Dat geldt bijvoorbeeld voor certificeringsstan...</td>\n",
       "      <td>[gelden, bijvoorbeeld, rtrs, nieuw, voorstelle...</td>\n",
       "      <td>gelden bijvoorbeeld rtrs nieuw voorstellen Eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15943</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>63</td>\n",
       "      <td>Zie hiervoor het antwoord op vraag 11.</td>\n",
       "      <td>[zien, hiervoor]</td>\n",
       "      <td>zien hiervoor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15944</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>65</td>\n",
       "      <td>Bent u van mening dat banken, pensioenfondsen ...</td>\n",
       "      <td>[mening, bank, pensioenfond, financieel, inste...</td>\n",
       "      <td>mening bank pensioenfond financieel instelling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15945</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>67</td>\n",
       "      <td>Banken, pensioenfondsen en andere financiële i...</td>\n",
       "      <td>[bank, pensioenfond, financieel, instelling, s...</td>\n",
       "      <td>bank pensioenfond financieel instelling spelen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15946</th>\n",
       "      <td>Antwoord op de vragen van het lid Tjeerd de Gr...</td>\n",
       "      <td>68</td>\n",
       "      <td>Vraag 15 Kunt u deze vragen elk afzonderlijk e...</td>\n",
       "      <td>[vragen, afzonderlijk, spoedig, mogelijk, bean...</td>\n",
       "      <td>vragen afzonderlijk spoedig mogelijk beantwoorden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15947 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title paragraph_num  \\\n",
       "0      Contaminatie in het vlees van ‘grote grazers’ ...             4   \n",
       "1      Contaminatie in het vlees van ‘grote grazers’ ...             7   \n",
       "2      Contaminatie in het vlees van ‘grote grazers’ ...             9   \n",
       "3      Contaminatie in het vlees van ‘grote grazers’ ...            10   \n",
       "4      Contaminatie in het vlees van ‘grote grazers’ ...            11   \n",
       "...                                                  ...           ...   \n",
       "15942  Antwoord op de vragen van het lid Tjeerd de Gr...            61   \n",
       "15943  Antwoord op de vragen van het lid Tjeerd de Gr...            63   \n",
       "15944  Antwoord op de vragen van het lid Tjeerd de Gr...            65   \n",
       "15945  Antwoord op de vragen van het lid Tjeerd de Gr...            67   \n",
       "15946  Antwoord op de vragen van het lid Tjeerd de Gr...            68   \n",
       "\n",
       "                                          paragraph_text  \\\n",
       "0      De Voorzitter van de Tweede Kamer der Staten-G...   \n",
       "1                                     : Parnassusplein 5   \n",
       "2                                   www.rijksoverheid.nl   \n",
       "3                            Kenmerk 3477122-1040635-VGP   \n",
       "4                                               Uw brief   \n",
       "...                                                  ...   \n",
       "15942  Dat geldt bijvoorbeeld voor certificeringsstan...   \n",
       "15943             Zie hiervoor het antwoord op vraag 11.   \n",
       "15944  Bent u van mening dat banken, pensioenfondsen ...   \n",
       "15945  Banken, pensioenfondsen en andere financiële i...   \n",
       "15946  Vraag 15 Kunt u deze vragen elk afzonderlijk e...   \n",
       "\n",
       "                                                 content  \\\n",
       "0                                 [voorzitter, generaal]   \n",
       "1                                       [parnassusplein]   \n",
       "2                                   [www, rijksoverheid]   \n",
       "3                                              [Kenmerk]   \n",
       "4                                                [brief]   \n",
       "...                                                  ...   \n",
       "15942  [gelden, bijvoorbeeld, rtrs, nieuw, voorstelle...   \n",
       "15943                                   [zien, hiervoor]   \n",
       "15944  [mening, bank, pensioenfond, financieel, inste...   \n",
       "15945  [bank, pensioenfond, financieel, instelling, s...   \n",
       "15946  [vragen, afzonderlijk, spoedig, mogelijk, bean...   \n",
       "\n",
       "                                                    corp  \n",
       "0                                    voorzitter generaal  \n",
       "1                                         parnassusplein  \n",
       "2                                      www rijksoverheid  \n",
       "3                                                Kenmerk  \n",
       "4                                                  brief  \n",
       "...                                                  ...  \n",
       "15942  gelden bijvoorbeeld rtrs nieuw voorstellen Eur...  \n",
       "15943                                      zien hiervoor  \n",
       "15944  mening bank pensioenfond financieel instelling...  \n",
       "15945  bank pensioenfond financieel instelling spelen...  \n",
       "15946  vragen afzonderlijk spoedig mogelijk beantwoorden  \n",
       "\n",
       "[15947 rows x 5 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee9b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTopic(language=\"dutch\")\n",
    "topics, probs = model.fit_transform(newDF['corp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71178d1",
   "metadata": {},
   "source": [
    "We can then extract the most and least frequent topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bcd5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43feec05",
   "metadata": {},
   "source": [
    "# **Visualize Topics**\n",
    "After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good \n",
    "understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. \n",
    "Instead, we can visualize the topics that were generated in a way very similar to \n",
    "[LDAvis](https://github.com/cpsievert/LDAvis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b269ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
